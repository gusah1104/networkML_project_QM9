{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "import assignment3_utils as utils\n",
    "from placeholder import PlaceHolder, to_dense, er_validation_step\n",
    "import placeholder\n",
    "import importlib\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.models import MLP, GIN, GAT\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from itertools import combinations\n",
    "from assignment3_utils import EigenFeatures\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool as gap\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "import random\n",
    "from transformer_model import GraphTransformer\n",
    "from assignment3_utils import NodeCycleFeatures\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import hmean\n",
    "import pickle\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "utils.seed_everything(0)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # cuda index to be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Loading total Dataset* ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset_np = np.load(\"./qm9_skeleton_9_nodes_345.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Utils for Forward Process* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "all_betas = utils.get_betas(timesteps=T, s=0.008)\n",
    "all_alphas = utils.get_alphas(timesteps=T)\n",
    "all_alphas_bar = utils.get_alphas_bar(all_alphas)\n",
    "n_nodes_dist = torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=torch.float64)\n",
    "\n",
    "def get_Q_t(betas: torch.Tensor, noise_dist: torch.Tensor, e_class=2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - betas, (bs, )\n",
    "        - noise_dist, (e_class, 1) -> the noise distribution at step $t=1,000$, here we take by default (0.5, 0.5)\n",
    "        - e_class, number of classes for edges\n",
    "    Output:\n",
    "        - q_e, (bs, e_class, e_class)\n",
    "    \"\"\"\n",
    "\n",
    "    q_e = torch.zeros(betas.shape[0], e_class, e_class)\n",
    "    for i in range(e_class):\n",
    "        q_e[:, i, i] = 1 - (betas*noise_dist.sum()).view(-1)\n",
    "    noise=noise_dist.unsqueeze(0).repeat(e_class,1)\n",
    "    for batch in range(betas.shape[0]):\n",
    "        q_e[batch,:,:]=q_e[batch,:,:]+betas[batch]*noise \n",
    "    assert ((q_e.sum(dim=2) - 1.0).abs() < 1e-4).all()  # ensure each row of q_e represents a distribution\n",
    "    return q_e\n",
    "\n",
    "def get_Q_t_bar(alphas_bar: torch.Tensor, noise_dist: torch.Tensor, e_class: int =2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - alphas_bar, (bs, )\n",
    "        - noise_dist, (e_class, 1) -> the noise distribution at step $t=1,000$, here we take by default (0.5, 0.5)\n",
    "        - e_class, number of classes for edges\n",
    "    Output:\n",
    "        - q_e, (bs, e_class, e_class)\n",
    "    \"\"\"\n",
    "\n",
    "    q_e = torch.zeros(alphas_bar.shape[0], e_class, e_class)\n",
    "    for i in range(e_class):\n",
    "        q_e[:, i, i] = alphas_bar.view(-1)\n",
    "    noise=noise_dist.unsqueeze(0).repeat(e_class,1)\n",
    "    for batch in range(alphas_bar.shape[0]):  \n",
    "        q_e[batch,:,:]=q_e[batch,:,:]+(1-alphas_bar[batch])*noise\n",
    "    assert ((q_e.sum(dim=2) - 1.0).abs() < 1e-4).all()  # ensure each row of q_e represents a distribution\n",
    "    return q_e\n",
    "\n",
    "def corrupt_edges(E: torch.Tensor, t_int: torch.Tensor, noise_dist: torch.Tensor, node_mask: torch.Tensor):\n",
    "    bs = E.size(0)\n",
    "    n = E.size(1)  # number of nodes\n",
    "    de = len(noise_dist)  # number of edge classes, i.e., 2\n",
    "\n",
    "    idx = t_int.to('cpu').squeeze(-1)\n",
    "    Qt_bar=get_Q_t_bar(all_alphas_bar[idx], noise_dist, de)\n",
    "    probE = torch.zeros(bs, n, n, de)\n",
    "    for i in range(bs):\n",
    "        probE[i] = E[i].matmul(Qt_bar[i])\n",
    "    \n",
    "    inverse_edge_mask = ~(node_mask.unsqueeze(1) * node_mask.unsqueeze(2))\n",
    "    diag_mask = torch.eye(n).unsqueeze(0).expand(bs, -1, -1)\n",
    "    probE[inverse_edge_mask] = 1 / probE.shape[-1]\n",
    "    probE[diag_mask.bool()] = 1 / probE.shape[-1]\n",
    "    probE = probE.reshape(bs * n * n, -1)    # (bs * n * n, de_out)\n",
    "    E_t = torch.multinomial(probE, 1).reshape(bs, n, n,1)\n",
    "    upper_triangle = torch.triu(E_t.squeeze(), diagonal=0)  \n",
    "    symmetric_matrix = upper_triangle + upper_triangle.transpose(1, 2) - torch.diag_embed(upper_triangle.diagonal(dim1=-2, dim2=-1))  \n",
    "    E_t = symmetric_matrix.unsqueeze(-1)\n",
    "    E_t= torch.nn.functional.one_hot(E_t, de).float().squeeze()\n",
    "    return E_t\n",
    "\n",
    "def apply_noise(holder, T, noise_dist, node_mask):\n",
    "    t_int = torch.randint(1, T + 1, size=(holder.E.size(0), 1))\n",
    "    E = holder.E\n",
    "    E_t = corrupt_edges(E, t_int, noise_dist, node_mask)\n",
    "    return PlaceHolder(X=holder.X, E=E_t, y=holder.y).mask(node_mask), t_int\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multiple GPUs\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Utils for Denoiser Model (graph transformer is in transformer_model.py)* ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_placeholder(holder):\n",
    "    E = torch.argmax(holder.E, dim=-1)\n",
    "    return PlaceHolder(X=holder.X, E=E, y=holder.y)\n",
    "\n",
    "def holder_to_data_batch(holder, node_mask):\n",
    "    # WARNING: holder should already be masked!\n",
    "    data_list = []\n",
    "    holder = collapse_placeholder(holder)\n",
    "\n",
    "    for graph_idx in range(holder.X.size(0)):\n",
    "        this_node_mask = node_mask[graph_idx]\n",
    "        n_nodes = this_node_mask.sum()\n",
    "        X = holder.X[graph_idx].squeeze()\n",
    "        X = X[this_node_mask]\n",
    "        E = holder.E[graph_idx]\n",
    "        edge_index, edge_attr = dense_to_sparse(adj=E[:n_nodes, :n_nodes])\n",
    "        y = holder.y[graph_idx]\n",
    "        data = Data(x=X, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        data.validate(raise_on_error=True)\n",
    "        data_list.append(data)\n",
    "\n",
    "        data_batch = Batch.from_data_list(data_list)\n",
    "\n",
    "    return data_batch\n",
    "\n",
    "class ER_prob_model():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        self.p = self.estimate_p(dataset)\n",
    "        self.n_dist = self.estimate_n_cat_dist(dataset)\n",
    "        print(f\"Estimated p: {self.p}\")\n",
    "        print(f\"Estimated n categorical distribution: {self.n_dist}\")\n",
    "\n",
    "    def estimate_n_cat_dist(self, dataset: List[nx.Graph]) -> Dict[int, float]:\n",
    "        \"\"\"Output must be a dictionary where keys are the number of nodes and values are the probability of that number of nodes.\"\"\"\n",
    "        n_dist = {}\n",
    "        for g in dataset:\n",
    "            n = g.number_of_nodes()\n",
    "            if n in n_dist:\n",
    "                n_dist[n] += 1\n",
    "            else:\n",
    "                n_dist[n] = 1\n",
    "        for n in n_dist:\n",
    "            n_dist[n] /= len(dataset)\n",
    "        return n_dist\n",
    "\n",
    "    def estimate_p(self, dataset: List[nx.Graph]) -> float:\n",
    "        \"\"\"Output must be a single floating point number representing the probability of an edge existing between two nodes.\"\"\"\n",
    "        p = sum([g.number_of_edges() for g in dataset]) / sum([g.number_of_nodes() * (g.number_of_nodes() - 1) / 2 for g in dataset])\n",
    "        return p\n",
    "\n",
    "    def sample_ER_graphs(self, n_graphs: int) -> List[nx.Graph]:\n",
    "        \"\"\"Output must be a list of n_graphs ER graphs sampled from the model. Note that you should use the estimated parameters, self.n_dist and self.p (see `fit` function).\"\"\"\n",
    "        graphs = []\n",
    "        for _ in range(n_graphs):\n",
    "            n = np.random.choice(list(self.n_dist.keys()), p=list(self.n_dist.values()))\n",
    "            graph = nx.Graph()\n",
    "            graph.add_nodes_from(range(n))\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    if np.random.rand() < self.p:\n",
    "                        graph.add_edge(i, j)\n",
    "            graphs.append(graph)\n",
    "        return graphs  \n",
    "    \n",
    "def generate_ER_graph(n: int, p_er: float) -> nx.Graph:\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if np.random.rand() < p_er:\n",
    "                graph.add_edge(i, j)\n",
    "    return graph\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dims: int,\n",
    "                 num_GNN_layers: int,\n",
    "                 hidden_dims: int,\n",
    "                 num_MLP_layers: int,\n",
    "                 hidden_MLP_dims: int,\n",
    "                 output_dims: int):\n",
    "        super().__init__()\n",
    "        self.gnn = GIN(\n",
    "            in_channels=input_dims,\n",
    "            out_channels=hidden_dims,\n",
    "            num_layers=num_GNN_layers,\n",
    "            hidden_channels=hidden_dims,\n",
    "            train_eps=False\n",
    "        )\n",
    "        self.mlp = MLP(\n",
    "            in_channels=hidden_dims,\n",
    "            hidden_channels=hidden_MLP_dims,\n",
    "            out_channels=output_dims,\n",
    "            num_layers=num_MLP_layers,\n",
    "            activation_layer=torch.nn.ReLU()\n",
    "            # activation_layer=None\n",
    "        )\n",
    "\n",
    "    def forward(self, holder: PlaceHolder, node_mask: torch.Tensor) -> PlaceHolder:\n",
    "        max_n_nodes = holder.E.shape[1]\n",
    "        bs=holder.X.size(0)\n",
    "        data = holder_to_data_batch(holder, node_mask)\n",
    "\n",
    "        # add timestep concatenating in node features\n",
    "        time = data.y\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        time_as_x = time[data.batch].unsqueeze(-1)\n",
    "        if x.dim() == 1:  \n",
    "            x = torch.hstack((x.unsqueeze(-1), time_as_x))  # if you don't use eigenfeatures\n",
    "        elif x.dim() == 2:\n",
    "            x = torch.hstack((x, time_as_x))  # if you  use eigenfeatures\n",
    "        node_embeddings = self.gnn(x[:,:-1], edge_index)\n",
    "        padded_node_embeddings = torch.zeros(bs, max_n_nodes, node_embeddings.size(-1))\n",
    "        total_sum=0\n",
    "        \n",
    "        for batch in range(bs):\n",
    "            padded_node_embeddings[batch, :sum(node_mask[batch]), :] = node_embeddings[total_sum:total_sum+sum(node_mask[batch])].view(sum(node_mask[batch]), -1)\n",
    "            total_sum+=sum(node_mask[batch])\n",
    "        node_embeddings = padded_node_embeddings\n",
    "        reshaped_original = node_embeddings[:,:, np.newaxis, :]\n",
    "        mlp_input_feature = reshaped_original * reshaped_original.swapaxes(1, 2)\n",
    "        mlp_input_feature = mlp_input_feature.view(-1, mlp_input_feature.size(-1))\n",
    "        output_E = self.mlp(mlp_input_feature)\n",
    "        output_E = output_E.view(-1,max_n_nodes,max_n_nodes,2)\n",
    "        output_E=utils.symmetrize(output_E)\n",
    "        output_holder = PlaceHolder(X=holder.X, E=output_E, y=None).mask(node_mask)\n",
    "\n",
    "        return output_holder\n",
    "    \n",
    "class SimpleModelGCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dims: int,\n",
    "                 num_GNN_layers: int,\n",
    "                 hidden_dims: int,\n",
    "                 num_MLP_layers: int,\n",
    "                 hidden_MLP_dims: int,\n",
    "                 output_dims: int):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dims, hidden_dims))\n",
    "        for _ in range(num_GNN_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dims, hidden_dims))\n",
    "        self.mlp = MLP(\n",
    "            in_channels=hidden_dims,\n",
    "            hidden_channels=hidden_MLP_dims,\n",
    "            out_channels=output_dims,\n",
    "            num_layers=num_MLP_layers,\n",
    "            # activation_layer=None\n",
    "            activation_layer=torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, holder: PlaceHolder, node_mask: torch.Tensor) -> PlaceHolder:\n",
    "        max_n_nodes = holder.E.shape[1]\n",
    "        bs = holder.X.size(0)\n",
    "        data = holder_to_data_batch(holder, node_mask)\n",
    "\n",
    "        # Add timestep concatenating in node features\n",
    "        time = data.y\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        time_as_x = time[data.batch].unsqueeze(-1)\n",
    "        if x.dim() == 1:  \n",
    "            x = torch.hstack((x.unsqueeze(-1), time_as_x))  # if you don't use eigenfeatures\n",
    "        elif x.dim() == 2:\n",
    "            x = torch.hstack((x, time_as_x))  # if you use eigenfeatures\n",
    "        \n",
    "        # Use the node features without the timestep for GNN\n",
    "        node_embeddings = x[:, :-1]\n",
    "        for conv in self.convs:\n",
    "            node_embeddings = conv(node_embeddings, edge_index).relu()\n",
    "        \n",
    "        padded_node_embeddings = torch.zeros(bs, max_n_nodes, node_embeddings.size(-1))\n",
    "        total_sum = 0\n",
    "\n",
    "        for batch in range(bs):\n",
    "            padded_node_embeddings[batch, :sum(node_mask[batch]), :] = node_embeddings[total_sum:total_sum + sum(node_mask[batch])].view(sum(node_mask[batch]), -1)\n",
    "            total_sum += sum(node_mask[batch])\n",
    "        node_embeddings = padded_node_embeddings\n",
    "        reshaped_original = node_embeddings[:, :, np.newaxis, :]\n",
    "        mlp_input_feature = reshaped_original * reshaped_original.swapaxes(1, 2)\n",
    "        mlp_input_feature = mlp_input_feature.view(-1, mlp_input_feature.size(-1))\n",
    "        output_E = self.mlp(mlp_input_feature)\n",
    "        output_E = output_E.view(-1, max_n_nodes, max_n_nodes, 2)\n",
    "        output_E = utils.symmetrize(output_E)\n",
    "        output_holder = PlaceHolder(X=holder.X, E=output_E, y=None).mask(node_mask)\n",
    "\n",
    "        return output_holder\n",
    "\n",
    "class SimpleModelGAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dims: int,\n",
    "                 num_GNN_layers: int,\n",
    "                 hidden_dims: int,\n",
    "                 num_MLP_layers: int,\n",
    "                 hidden_MLP_dims: int,\n",
    "                 output_dims: int,\n",
    "                 heads: int = 1):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATConv(input_dims, hidden_dims, heads=heads))\n",
    "        for _ in range(num_GNN_layers - 1):\n",
    "            self.convs.append(GATConv(hidden_dims * heads, hidden_dims, heads=heads))\n",
    "        self.mlp = MLP(\n",
    "            in_channels=hidden_dims * heads,\n",
    "            hidden_channels=hidden_MLP_dims,\n",
    "            out_channels=output_dims,\n",
    "            num_layers=num_MLP_layers,\n",
    "            # activation_layer=None\n",
    "            activation_layer=torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, holder: PlaceHolder, node_mask: torch.Tensor) -> PlaceHolder:\n",
    "        max_n_nodes = holder.E.shape[1]\n",
    "        bs = holder.X.size(0)\n",
    "        data = holder_to_data_batch(holder, node_mask)\n",
    "\n",
    "        # Add timestep concatenating in node features\n",
    "        time = data.y\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        time_as_x = time[data.batch].unsqueeze(-1)\n",
    "        if x.dim() == 1:  \n",
    "            x = torch.hstack((x.unsqueeze(-1), time_as_x))  # if you don't use eigenfeatures\n",
    "        elif x.dim() == 2:\n",
    "            x = torch.hstack((x, time_as_x))  # if you use eigenfeatures\n",
    "        \n",
    "        # Use the node features without the timestep for GNN\n",
    "        node_embeddings = x[:, :-1]\n",
    "        for conv in self.convs:\n",
    "            node_embeddings = conv(node_embeddings, edge_index).relu()\n",
    "        \n",
    "        padded_node_embeddings = torch.zeros(bs, max_n_nodes, node_embeddings.size(-1))\n",
    "        total_sum = 0\n",
    "\n",
    "        for batch in range(bs):\n",
    "            padded_node_embeddings[batch, :sum(node_mask[batch]), :] = node_embeddings[total_sum:total_sum + sum(node_mask[batch])].view(sum(node_mask[batch]), -1)\n",
    "            total_sum += sum(node_mask[batch])\n",
    "        node_embeddings = padded_node_embeddings\n",
    "        reshaped_original = node_embeddings[:, :, np.newaxis, :]\n",
    "        mlp_input_feature = reshaped_original * reshaped_original.swapaxes(1, 2)\n",
    "        mlp_input_feature = mlp_input_feature.view(-1, mlp_input_feature.size(-1))\n",
    "        output_E = self.mlp(mlp_input_feature)\n",
    "        output_E = output_E.view(-1, max_n_nodes, max_n_nodes, 2)\n",
    "        output_E = utils.symmetrize(output_E)\n",
    "        output_holder = PlaceHolder(X=holder.X, E=output_E, y=None).mask(node_mask)\n",
    "\n",
    "        return output_holder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Utils for Training* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(seed, model, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr=2e-4):\n",
    "    # Training parameters\n",
    "    seed_torch(seed)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    val_loss_list = []\n",
    "    loss_list = []\n",
    "    val_metric = torch.nn.CrossEntropyLoss()\n",
    "    train_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Iterate over the batches\n",
    "    for epoch_idx in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            # Access the batch data\n",
    "            unmasked_holder, node_mask = to_dense(batch)\n",
    "            noise_holder, t_int = apply_noise(unmasked_holder, T, noise_dist, node_mask)\n",
    "            target_holder = unmasked_holder.mask(node_mask)\n",
    "            # Prepare data for inference\n",
    "            noise_holder.y = t_int.float() / T\n",
    "            noise_holder = noise_holder.to(device)\n",
    "            node_mask = node_mask.to(device)\n",
    "            target_holder = target_holder.to(device)\n",
    "\n",
    "            if eigen_feats is not None:\n",
    "                add_feats = eigen_feats(noise_holder.E, node_mask)\n",
    "                cycle_features = NodeCycleFeatures()\n",
    "                x_cycles, y_cycles = cycle_features(noise_holder,node_mask)\n",
    "   \n",
    "                noise_holder.X = torch.cat((noise_holder.X, x_cycles,add_feats[2], add_feats[3]), -1)\n",
    "                noise_holder.y = torch.cat((noise_holder.y, y_cycles,add_feats[0], add_feats[1]), -1)\n",
    "                \n",
    "            pred = model(noise_holder, node_mask)\n",
    "            E_true = target_holder.E.reshape(-1, target_holder.E.shape[-1])\n",
    "            E_pred = pred.E.reshape(-1, pred.E.shape[-1])\n",
    "            loss = train_loss(E_pred, E_true)\n",
    "     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss.detach().cpu().item()           \n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        loss_list.append(np.mean(epoch_loss))\n",
    "        val_loss = validation_step(val_dataloader, val_metric, model, noise_dist, T, eigen_feats)\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "\n",
    "    # Baseline\n",
    "    er_train_loss = er_validation_step(train_dataloader,val_metric, er_model)\n",
    "    er_train_loss_arr = er_train_loss * np.ones(n_epochs)\n",
    "    er_val_loss = er_validation_step(val_dataloader,val_metric, er_model)\n",
    "    er_val_loss_arr = er_val_loss * np.ones(n_epochs)\n",
    "    print(epoch_loss[-1])\n",
    "\n",
    "    plt.plot(loss_list, label=\"train_loss\")\n",
    "    window_size = 10\n",
    "    plt.plot(np.convolve(loss_list, np.ones(window_size)/window_size, mode='valid'), label=\"smooth train\")\n",
    "    plt.plot(val_loss_list, label=\"val_loss\")\n",
    "    plt.plot(er_train_loss_arr, label=\"er_train_loss\")\n",
    "    plt.plot(er_val_loss_arr, label=\"er_val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model, loss_list, val_loss_list, er_train_loss, er_val_loss\n",
    "\n",
    "def validation_step(val_dataloader, val_metric, model, noise_dist, T, eigen_feats):\n",
    "    model.eval()\n",
    "\n",
    "    # Get val loss\n",
    "    loss_list = []\n",
    "    for batch in val_dataloader:\n",
    "        # Access the batch data\n",
    "        unmasked_holder, node_mask = to_dense(batch)\n",
    "        noise_holder, t_int = apply_noise(unmasked_holder, T, noise_dist, node_mask)\n",
    "        target_holder = unmasked_holder.mask(node_mask)\n",
    "\n",
    "        # Prepare data for inference\n",
    "        noise_holder.y = t_int.float() / T\n",
    "        noise_holder = noise_holder.to(device)\n",
    "        node_mask = node_mask.to(device)\n",
    "        target_holder = target_holder.to(device)\n",
    "\n",
    "        # get values for training model\n",
    "        if eigen_feats is not None:\n",
    "            add_feats = eigen_feats(noise_holder.E, node_mask)\n",
    "            cycle_features = NodeCycleFeatures()\n",
    "            x_cycles, y_cycles = cycle_features(noise_holder,node_mask)\n",
    "            noise_holder.X = torch.cat((noise_holder.X, x_cycles,add_feats[2], add_feats[3]), -1)\n",
    "            noise_holder.y = torch.cat((noise_holder.y, y_cycles,add_feats[0], add_feats[1]), -1)\n",
    "\n",
    "        pred = model(noise_holder, node_mask)\n",
    "        E_true = target_holder.E.reshape(-1, target_holder.E.shape[-1])\n",
    "        E_pred = pred.E.reshape(-1, pred.E.shape[-1])\n",
    "        val_loss = val_metric(E_pred, E_true)\n",
    "\n",
    "\n",
    "        loss_list.append(val_loss.detach().cpu().numpy())\n",
    "\n",
    "    val_loss = sum(loss_list)/len(loss_list)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "def sample_graph(seed, model, noise_dist, T, n_graphs, num_timesteps_to_save=5, eigen_feats=None):\n",
    "    seed_torch(seed)\n",
    "    max_n_nodes = len(n_nodes_dist) - 1\n",
    "    de = len(noise_dist)\n",
    "\n",
    "    n_nodes = torch.multinomial(n_nodes_dist, n_graphs, replacement=True) \n",
    "    node_mask=torch.zeros(n_graphs, max_n_nodes).bool()\n",
    "    for i in range(n_graphs):\n",
    "        node_mask[i, :n_nodes[i]] = True\n",
    "\n",
    "    # sample a purely noised graph at step T\n",
    "    limit_dist = noise_dist.repeat(n_graphs * max_n_nodes * max_n_nodes, 1)\n",
    "    limit_E = limit_dist.multinomial(1, replacement=True)\n",
    "    limit_E = limit_E.reshape(n_graphs, max_n_nodes, max_n_nodes)\n",
    "    limit_E = torch.nn.functional.one_hot(limit_E, 2).float()\n",
    "    limit_E = utils.symmetrize(limit_E)\n",
    "\n",
    "    holder = PlaceHolder(X=torch.ones(n_graphs, max_n_nodes, 1),\n",
    "                         E=limit_E,\n",
    "                         y=torch.Tensor([1]).unsqueeze(-1).repeat(n_graphs, 1),).to(device).mask(node_mask)\n",
    "\n",
    "    saving_steps = torch.linspace(0, T, num_timesteps_to_save).round()\n",
    "    saved_holders = [holder.to('cpu')]  # save initial holder\n",
    "    print(\"Saving timesteps: \", saving_steps)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in tqdm(range(1, T+1)[::-1]):\n",
    "            steps = torch.Tensor([t]).unsqueeze(-1).repeat(n_graphs, 1).to(device)\n",
    "            steps_next = steps - 1\n",
    "            holder.y = steps / T\n",
    "\n",
    "            # if eigen_feats is not None:\n",
    "            #     add_feats = eigen_feats(holder.E, node_mask)\n",
    "            #     holder.X = torch.cat((holder.X, add_feats[2], add_feats[3]), -1)\n",
    "            #     holder.y = torch.cat((holder.y, add_feats[0], add_feats[1]), -1)\n",
    "            if eigen_feats is not None:\n",
    "                add_feats = eigen_feats(holder.E, node_mask)\n",
    "                cycle_features = NodeCycleFeatures()\n",
    "                x_cycles, y_cycles = cycle_features(holder,node_mask)\n",
    "                holder.X = torch.cat((holder.X, x_cycles,add_feats[2], add_feats[3]), -1)\n",
    "                holder.y = torch.cat((holder.y, y_cycles,add_feats[0], add_feats[1]), -1)\n",
    "\n",
    "            pred_G0 = model(holder, node_mask)\n",
    "            pred_E = torch.softmax(pred_G0.E, dim=-1)\n",
    "            shape_E = pred_E.shape\n",
    "            E_t = holder.E.to(torch.float32)              # bs, n, n, 2\n",
    "\n",
    "            # transition matrix used in the reverse process of size (bs, 2, 2)\n",
    "            Qtb = get_Q_t_bar(all_alphas_bar[steps[:, 0].long().to('cpu')], noise_dist, e_class=2).to(device)\n",
    "            Qsb = get_Q_t_bar(all_alphas_bar[steps_next[:, 0].long().to('cpu')], noise_dist, e_class=2).to(device)\n",
    "            Qt = get_Q_t(all_betas[steps[:, 0].long().to('cpu')], noise_dist, e_class=2).to(device)\n",
    "\n",
    "            # posterior computation\n",
    "            E_t = E_t.flatten(start_dim=1, end_dim=-2).to(torch.float32)            # bs x N x dt\n",
    "\n",
    "            Qt_T = Qt.transpose(-1, -2)                 # bs, dt, d_t-1\n",
    "            left_term = E_t @ Qt_T                      # bs, N, d_t-1\n",
    "            left_term = left_term.unsqueeze(dim=2)      # bs, N, 1, d_t-1\n",
    "\n",
    "            right_term = Qsb.unsqueeze(1)               # bs, 1, d0, d_t-1\n",
    "            numerator = left_term * right_term          # bs, N, d0, d_t-1\n",
    "\n",
    "            X_t_transposed = E_t.transpose(-1, -2)      # bs, dt, N\n",
    "\n",
    "            prod = Qtb @ X_t_transposed                 # bs, d0, N\n",
    "            prod = prod.transpose(-1, -2)               # bs, N, d0\n",
    "            denominator = prod.unsqueeze(-1)            # bs, N, d0, 1\n",
    "            denominator[denominator == 0] = 1e-6\n",
    "\n",
    "            p_s_and_t_given_0_E = numerator / denominator\n",
    "\n",
    "            # noise back\n",
    "            pred_E = pred_E.reshape((n_graphs, -1, pred_E.shape[-1]))\n",
    "            weighted_E = pred_E.unsqueeze(-1) * p_s_and_t_given_0_E        # bs, N, d0, d_t-1\n",
    "            unnormalized_prob_E = weighted_E.sum(dim=-2)\n",
    "            unnormalized_prob_E[torch.sum(unnormalized_prob_E, dim=-1) == 0] = 1e-5\n",
    "            prob_E = unnormalized_prob_E / torch.sum(unnormalized_prob_E, dim=-1, keepdim=True)\n",
    "            prob_E = prob_E.reshape(n_graphs, max_n_nodes, max_n_nodes, pred_E.shape[-1])\n",
    "\n",
    "            # sample at timestep t-1\n",
    "            E_s = prob_E.reshape(-1, de).multinomial(1, replacement=True)\n",
    "            E_s = torch.nn.functional.one_hot(E_s, 2).float().reshape(shape_E)\n",
    "            E_s = utils.symmetrize(E_s)\n",
    "\n",
    "            assert (E_s == torch.transpose(E_s, 1, 2)).all()\n",
    "            assert (E_s.shape == E_s.shape)\n",
    "            holder = PlaceHolder(X=torch.ones(n_graphs, max_n_nodes, 1).to(device),\n",
    "                                E=E_s, y=steps_next/T).to(device).mask(node_mask)\n",
    "            # Save graphs\n",
    "            if t-1 in saving_steps:\n",
    "                saved_holders.append(holder.to('cpu'))\n",
    "\n",
    "    return holder, saved_holders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Utils for Evaluation* ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_hist_array(nx_dataset: List[nx.Graph], max_degree: int =20) -> np.ndarray:\n",
    "    \"\"\"The output should be an np.array of shape (len(nx_dataset), max_degree),  where each row corresponds to the degree histogram obtained for a different graph of the dataset.\"\"\"\n",
    "    deg_hist_array = np.zeros((len(nx_dataset), max_degree))\n",
    "    for i, g in enumerate(nx_dataset):\n",
    "        deg = np.array([g.degree(node) for node in g.nodes()])\n",
    "        deg_hist, _ = np.histogram(deg, bins=np.arange(max_degree+1))\n",
    "        deg_hist_array[i] = deg_hist\n",
    "    return deg_hist_array\n",
    "\n",
    "def get_clustering_hist_array(nx_dataset, num_bins=10) -> np.ndarray:\n",
    "    \"\"\"The output should be an np.array of shape (len(nx_dataset), num_bins), where each row corresponds to the node cluster coefficient histogram obtained for a different graph of the dataset.\"\"\"\n",
    "    clustering_hist_array = np.zeros((len(nx_dataset), num_bins))\n",
    "    for i, g in enumerate(nx_dataset):\n",
    "        clustering = np.array(list(nx.clustering(g).values()))\n",
    "        clustering_hist, _ = np.histogram(clustering, bins=np.linspace(0, 1, num_bins+1))\n",
    "        clustering_hist_array[i] = clustering_hist\n",
    "    return clustering_hist_array\n",
    "\n",
    "def is_valid_graph(g: nx.Graph) -> bool:\n",
    "    \"\"\"Check if the graph is valid based on the given criteria.\"\"\"\n",
    "    # Check if the graph has exactly one connected component\n",
    "    if not nx.is_connected(g):\n",
    "        return False\n",
    "    \n",
    "    # Check if every node has an edge number between 1 and 4\n",
    "    for node in g.nodes():\n",
    "        if not (1 <= g.degree(node) <= 4):\n",
    "            return False\n",
    "    \n",
    "    # Check if the graph is planar\n",
    "    is_planar, _ = nx.check_planarity(g)\n",
    "    if not is_planar:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def count_valid_graphs(nx_dataset: List[nx.Graph]) -> int:\n",
    "    \"\"\"Count the number of valid graphs in the list based on the criteria.\"\"\"\n",
    "    valid_count = 0\n",
    "    for g in nx_dataset:\n",
    "        if is_valid_graph(g):\n",
    "            valid_count += 1\n",
    "    return valid_count\n",
    "\n",
    "def graph_to_canonical_form(graph: nx.Graph) -> Tuple:\n",
    "    \"\"\"Convert a graph to a canonical form using sorted adjacency list.\"\"\"\n",
    "    adjacency_list = nx.to_dict_of_lists(graph)\n",
    "    for key in adjacency_list:\n",
    "        adjacency_list[key].sort()\n",
    "    return tuple(sorted((k, tuple(v)) for k, v in adjacency_list.items()))\n",
    "\n",
    "def count_unique_and_novel_graphs(graph_list: List[nx.Graph], train_set: List[nx.Graph]) -> Tuple[int, int]:\n",
    "    \"\"\"Count the number of unique graphs and the number of novel graphs from the train set.\"\"\"\n",
    "    train_canonical_forms = {graph_to_canonical_form(g) for g in train_set}\n",
    "    graph_canonical_forms = {graph_to_canonical_form(g) for g in graph_list}\n",
    "    \n",
    "    unique_graphs = len(graph_canonical_forms)\n",
    "    novel_graphs = len(graph_canonical_forms - train_canonical_forms)\n",
    "    \n",
    "    return unique_graphs, novel_graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Actual Training, and generating graphs* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_to_eval= {}\n",
    "train_loss_list = {}\n",
    "val_loss_list = {}\n",
    "er_train_loss_list = {}\n",
    "er_val_loss_list = {}\n",
    "\n",
    "trial_num=5\n",
    "for i in range(trial_num):\n",
    "        seed=i\n",
    "        utils.seed_everything(seed)\n",
    "        # [0.5,0.5] without eigenfeatures\n",
    "        noise_dist = torch.Tensor([0.5, 0.5])\n",
    "\n",
    "        train_indices = np.random.choice(len(total_dataset_np), 250, replace=False)\n",
    "        train_dataset_np = total_dataset_np[train_indices]\n",
    "        test_dateset_np = total_dataset_np[np.delete(np.arange(len(total_dataset_np)), train_indices)] # get the rest as test dataset\n",
    "        nx_dataset={}\n",
    "        nx_dataset[\"train\"]=[nx.from_numpy_matrix(data) for data in train_dataset_np]\n",
    "        nx_dataset[\"test\"]=[nx.from_numpy_matrix(data) for data in test_dateset_np]\n",
    "        train_dataset = nx_dataset[\"train\"]\n",
    "        \n",
    "        graphs_to_eval[f\"train_{i}\"] = train_dataset\n",
    "        graphs_to_eval[f\"test_{i}\"] = nx_dataset[\"test\"]\n",
    "        num_graphs_to_generate = 20\n",
    "        bs = 32\n",
    "        train_dataloader = utils.nx_list_to_dataloader(nx_dataset[\"train\"], bs=bs)\n",
    "        val_dataloader = utils.nx_list_to_dataloader(nx_dataset[\"test\"], bs=bs)\n",
    "        test_dataloader = utils.nx_list_to_dataloader(nx_dataset[\"test\"], bs=bs)\n",
    "\n",
    "        def count_n_node(dataset: List[nx.Graph]) -> np.ndarray:\n",
    "                \"\"\"Output must be a numpy array of shape (max_n_nodes + 1,) containing the probability of each number of nodes in the dataset, where max_n_nodes is the highest number of nodes found for a single graph in the dataset.\"\"\"\n",
    "                max_n_nodes=max([g.number_of_nodes() for g in dataset])\n",
    "                prob = np.zeros(max_n_nodes + 1)\n",
    "                for g in dataset:\n",
    "                        prob[g.number_of_nodes()] += 1\n",
    "                prob /= len(dataset)\n",
    "                return prob\n",
    "\n",
    "        n_nodes_dist = torch.tensor(count_n_node(nx_dataset['train']))\n",
    "        max_n_nodes = len(n_nodes_dist)-1\n",
    "\n",
    "\n",
    "        er_model = ER_prob_model()\n",
    "        er_model.fit(train_dataset)\n",
    "        \n",
    "        graphs_to_eval[f\"ER_{i}\"] = er_model.sample_ER_graphs(num_graphs_to_generate)\n",
    "\n",
    "        n_epochs = 20 # you can try epoches if you want\n",
    "\n",
    "        args_simple = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 1,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model = SimpleModel(**args_simple).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        # eigen_feats = utils.EigenFeatures('all')\n",
    "        eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model,train_loss,val_loss,er_train_loss,er_val_loss = train_model(seed, simple_model, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_{i}\"] = val_loss\n",
    "        er_train_loss_list[f\"simple_{i}\"] = er_train_loss\n",
    "        er_val_loss_list[f\"simple_{i}\"] = er_val_loss\n",
    "        simple_model_graphs, simple_model_intermediate_graphs = sample_graph(seed, simple_model, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_{i}\"] = simple_model_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "\n",
    "        args_simple_gcn = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 1,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gcn = SimpleModelGCN(**args_simple_gcn).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        # eigen_feats = utils.EigenFeatures('all')\n",
    "        eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_gcn,train_loss,val_loss, _, _ = train_model(seed, simple_model_gcn, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gcn_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gcn_{i}\"] = val_loss\n",
    "        simple_model_gcn_graphs, simple_model_gcn_intermediate_graphs = sample_graph(seed, simple_model_gcn, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=None)\n",
    "        graphs_to_eval[f\"simple_gcn_{i}\"] = simple_model_gcn_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_simple_gat = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 1,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gat = SimpleModelGAT(**args_simple_gat).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        # eigen_feats = utils.EigenFeatures('all')\n",
    "        eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_gat, train_loss, val_loss, _,_ = train_model(seed, simple_model_gat, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gat_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gat_{i}\"] = val_loss\n",
    "        simple_model_gat_graphs, simple_model_gat_intermediate_graphs = sample_graph(seed, simple_model_gat, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_gat_{i}\"] = simple_model_gat_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_gt = {'n_layers': 4,\n",
    "                'n_head': 8,\n",
    "                'input_dims': {'X': 1, 'E': 2, 'y': 1},\n",
    "                'hidden_dims': {'X': 256, 'E': 256, 'y': 256, 'dx': 64, 'de': 64, 'dy': 64},\n",
    "                'output_dims': {'X': 1, 'E': 2, 'y': 1},}\n",
    "        n_epochs = 5\n",
    "        gt_model = GraphTransformer(**args_gt).to(device)\n",
    "        lr=2e-4\n",
    "        gt_model,train_loss,val_loss,_,_ = train_model(seed, gt_model, train_dataloader, val_dataloader, n_epochs, noise_dist, T, None,lr)\n",
    "        train_loss_list[f\"gt_{i}\"] = train_loss\n",
    "        val_loss_list[f\"gt_{i}\"] = val_loss\n",
    "        num_graphs_to_generate = len(nx_dataset[\"test\"])\n",
    "        gt_model_graphs, gt_model_intermediate_graphs = sample_graph(seed, gt_model, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=None)\n",
    "        graphs_to_eval[f\"gt_{i}\"] = gt_model_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #[0.5,0,5] with eigenfeatures\n",
    "        noise_dist = torch.Tensor([0.5, 0.5])\n",
    "\n",
    "        n_epochs = 100 # you can try epoches if you want\n",
    "\n",
    "        args_simple = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 7,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_eigen = SimpleModel(**args_simple).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        # eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_eigen, train_loss, val_loss, _, _ = train_model(seed, simple_model_eigen, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_eigen_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_eigen_{i}\"] = val_loss\n",
    "        simple_model_eigen_graphs, simple_model_eigen_intermediate_graphs = sample_graph(seed, simple_model_eigen, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_eigen_{i}\"] = simple_model_eigen_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "\n",
    "        args_simple_gcn = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 7,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gcn_eigen = SimpleModelGCN(**args_simple_gcn).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        #     eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_gcn_eigen, train_loss, val_loss,_,_ = train_model(seed, simple_model_gcn_eigen, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gcn_eigen_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gcn_eigen_{i}\"] = val_loss\n",
    "        simple_model_gcn_eigen_graphs, simple_model_gcn_eigen_intermediate_graphs = sample_graph(seed, simple_model_gcn_eigen, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_gcn_eigen_{i}\"] = simple_model_gcn_eigen_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_simple_gat = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 7,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gat_eigen = SimpleModelGAT(**args_simple_gat).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        # eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_gat_eigen,train_loss, val_loss,_,_ = train_model(seed, simple_model_gat_eigen, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gat_eigen_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gat_eigen_{i}\"] = val_loss\n",
    "        simple_model_gat_eigen_graphs, simple_model_gat_eigen_intermediate_graphs = sample_graph(seed, simple_model_gat_eigen, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_gat_eigen_{i}\"] = simple_model_gat_eigen_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_gt_pp = {'n_layers': 4,\n",
    "        'n_head': 8,\n",
    "        'input_dims': {'X': 7, 'E': 2, 'y': 7},\n",
    "        'hidden_dims': {'X': 256, 'E': 256, 'y': 256, 'dx': 64, 'de': 64, 'dy': 64},\n",
    "        'output_dims': {'X': 1, 'E': 2, 'y': 1},}\n",
    "        n_epochs=5\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        gt_pp_model = GraphTransformer(**args_gt_pp).to(device)\n",
    "        lr=2e-4\n",
    "        gt_pp_model,train_loss,val_loss,_,_ = train_model(seed, gt_pp_model, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"gt_eigen_{i}\"] = train_loss\n",
    "        val_loss_list[f\"gt_eigen_{i}\"] = val_loss\n",
    "        gt_pp_model_graphs, gt_pp_model_intermediate_graphs = sample_graph(seed, gt_pp_model, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"gt_eigen_{i}\"] = gt_pp_model_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Marginal without eigenfeatures\n",
    "        noise_dist= torch.Tensor([1-0.30574074074074076, 0.30574074074074076]) \n",
    "        n_epochs = 100 # you can try epoches if you want\n",
    "\n",
    "        args_simple = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 1,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_marginal = SimpleModel(**args_simple).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        #     eigen_feats = utils.EigenFeatures('all')\n",
    "        eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_marginal,train_loss,val_loss,_,_ = train_model(seed, simple_model_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_marginal_{i}\"] = val_loss\n",
    "        simple_model_marginal_graphs, simple_model_marginal_intermediate_graphs = sample_graph(seed, simple_model_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_marginal_{i}\"] = simple_model_marginal_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "\n",
    "        args_simple_gcn = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 1,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gcn_marginal = SimpleModelGCN(**args_simple_gcn).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        # eigen_feats = utils.EigenFeatures('all')\n",
    "        eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_gcn_marginal,train_loss,val_loss,_,_ = train_model(seed, simple_model_gcn_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gcn_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gcn_marginal_{i}\"] = val_loss\n",
    "        simple_model_gcn_marginal_graphs, simple_model_gcn_marginal_intermediate_graphs = sample_graph(seed, simple_model_gcn_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=None)\n",
    "        graphs_to_eval[f\"simple_gcn_marginal_{i}\"] = simple_model_gcn_marginal_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_simple_gat = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 1,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gat_marginal = SimpleModelGAT(**args_simple_gat).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        #     eigen_feats = utils.EigenFeatures('all')\n",
    "        eigen_feats= None\n",
    "        plt.ylim(0.090,0.12)\n",
    "        simple_model_gat_marginal,train_loss,val_loss,_,_ = train_model(seed, simple_model_gat_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gat_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gat_marginal_{i}\"] = val_loss\n",
    "        simple_model_gat_marginal_graphs, simple_model_gat_marginal_intermediate_graphs = sample_graph(seed, simple_model_gat_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_gat_marginal_{i}\"] = simple_model_gat_marginal_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_gt = {'n_layers': 4,\n",
    "                'n_head': 8,\n",
    "                'input_dims': {'X': 1, 'E': 2, 'y': 1},\n",
    "                'hidden_dims': {'X': 256, 'E': 256, 'y': 256, 'dx': 64, 'de': 64, 'dy': 64},\n",
    "                'output_dims': {'X': 1, 'E': 2, 'y': 1},}\n",
    "        n_epochs = 5\n",
    "        gt_model_marginal = GraphTransformer(**args_gt).to(device)\n",
    "        lr=2e-4\n",
    "        gt_model_marginal,train_loss,val_loss,_,_ = train_model(seed, gt_model_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, None,lr)\n",
    "        train_loss_list[f\"gt_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"gt_marginal_{i}\"] = val_loss\n",
    "        num_graphs_to_generate = len(nx_dataset[\"test\"])\n",
    "        gt_model_marginal_graphs, gt_model_marginal_intermediate_graphs = sample_graph(seed, gt_model_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=None)\n",
    "        graphs_to_eval[f\"gt_marginal_{i}\"] = gt_model_marginal_graphs.to_nx_graph_list()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #marginal with eigenfeatures\n",
    "        noise_dist = torch.Tensor([1-0.30574074074074076, 0.30574074074074076]) \n",
    "\n",
    "        n_epochs = 100 # you can try epoches if you want\n",
    "\n",
    "        args_simple = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 7,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_eigen_marginal = SimpleModel(**args_simple).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        # eigen_feats= None\n",
    "        # plt.ylim(0.090,0.12)\n",
    "        simple_model_eigen_marginal,train_loss,val_loss,_,_ = train_model(seed, simple_model_eigen_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_eigen_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_eigen_marginal_{i}\"] = val_loss\n",
    "        simple_model_eigen_marginal_graphs, simple_model_eigen_marginal_intermediate_graphs = sample_graph(seed, simple_model_eigen_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_eigen_marginal_{i}\"] = simple_model_eigen_marginal_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "\n",
    "        args_simple_gcn = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 7,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gcn_eigen_marginal = SimpleModelGCN(**args_simple_gcn).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        #     eigen_feats= None\n",
    "        # plt.ylim(0.090,0.12)\n",
    "        simple_model_gcn_eigen_marginal,train_loss,val_loss,_,_ = train_model(seed, simple_model_gcn_eigen_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gcn_eigen_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gcn_eigen_marginal_{i}\"] = val_loss\n",
    "        simple_model_gcn_eigen_marginal_graphs, simple_model_gcn_eigen_marginal_intermediate_graphs = sample_graph(seed, simple_model_gcn_eigen_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_gcn_eigen_marginal_{i}\"] = simple_model_gcn_eigen_marginal_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_simple_gat = {'num_MLP_layers': 2,\n",
    "                'num_GNN_layers': 2,\n",
    "                'input_dims': 7,            # 1 if you dont use eigenfeature, 7 if you use eigenfeatures\n",
    "                'hidden_dims': 100,\n",
    "                'hidden_MLP_dims': 100,\n",
    "                'output_dims': 2}\n",
    "        simple_model_gat_eigen_marginal = SimpleModelGAT(**args_simple_gat).to(device)\n",
    "        lr=2e-3      #1e-3 if you use eigenfeatures, 2e-4 if you dont use eigenfeatures\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        # eigen_feats= None\n",
    "        # plt.ylim(0.090,0.12)\n",
    "        simple_model_gat_eigen_marginal,train_loss,val_loss,_,_ = train_model(seed, simple_model_gat_eigen_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"simple_gat_eigen_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"simple_gat_eigen_marginal_{i}\"] = val_loss\n",
    "        simple_model_gat_eigen_marginal_graphs, simple_model_gat_eigen_marginal_intermediate_graphs = sample_graph(seed, simple_model_gat_eigen_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"simple_gat_eigen_marginal_{i}\"] = simple_model_gat_eigen_marginal_graphs.to_nx_graph_list()\n",
    "\n",
    "\n",
    "        args_gt_pp = {'n_layers': 4,\n",
    "        'n_head': 8,\n",
    "        'input_dims': {'X': 7, 'E': 2, 'y': 7},\n",
    "        'hidden_dims': {'X': 256, 'E': 256, 'y': 256, 'dx': 64, 'de': 64, 'dy': 64},\n",
    "        'output_dims': {'X': 1, 'E': 2, 'y': 1},}\n",
    "        n_epochs=5\n",
    "        eigen_feats = utils.EigenFeatures('all')\n",
    "        gt_pp_model_marginal = GraphTransformer(**args_gt_pp).to(device)\n",
    "        lr=2e-4\n",
    "        gt_pp_model_marginal,train_loss,val_loss,_,_ = train_model(seed, gt_pp_model_marginal, train_dataloader, val_dataloader, n_epochs, noise_dist, T, eigen_feats,lr)\n",
    "        train_loss_list[f\"gt_eigen_marginal_{i}\"] = train_loss\n",
    "        val_loss_list[f\"gt_eigen_marginal_{i}\"] = val_loss\n",
    "        gt_pp_model_marginal_graphs, gt_pp_model_marginal_intermediate_graphs = sample_graph(seed, gt_pp_model_marginal, noise_dist, T, num_graphs_to_generate, num_timesteps_to_save=5, eigen_feats=eigen_feats)\n",
    "        graphs_to_eval[f\"gt_eigen_marginal_{i}\"] = gt_pp_model_marginal_graphs.to_nx_graph_list()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#Save learning curve and generated graphs\n",
    "\n",
    "with open('graphs_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(graphs_to_eval, f)\n",
    "with open('train_loss_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(train_loss_list, f)\n",
    "with open('val_loss_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(val_loss_list,f)\n",
    "with open('er_train_loss_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(er_train_loss_list, f)\n",
    "with open('er_val_loss_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(er_val_loss_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Plotting Learning Curve* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved learning curve and generated graphs\n",
    "\n",
    "def load_pickle(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        data = {}  \n",
    "    return data\n",
    "\n",
    "\n",
    "graphs_dict = load_pickle('graphs_dict_merged.pkl')\n",
    "train_loss_dict = load_pickle('train_loss_dict_merged.pkl')\n",
    "val_loss_dict = load_pickle('val_loss_dict_merged.pkl')\n",
    "er_train_loss_dict = load_pickle('er_train_loss_dict_merged.pkl')\n",
    "er_val_loss_dict = load_pickle('er_val_loss_dict_merged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries\n",
    "average_train_loss = {}\n",
    "average_val_loss = {}\n",
    "std_train_loss = {}\n",
    "std_val_loss = {}\n",
    "average_er_train_loss = {}\n",
    "average_er_val_loss = {}\n",
    "\n",
    "# Compute means and standard deviations for different configurations\n",
    "configs = [\"simple\", \"simple_eigen\",\"simple_eigen_cycles\", \"simple_marginal\", \"simple_eigen_marginal\", \"simple_eigen_marginal_cycles\", \"simple_gcn\", \"simple_gcn_eigen\", \"simple_gcn_marginal\", \"simple_gcn_eigen_marginal\",\"simple_gat\", \"simple_gat_eigen\", \"simple_gat_marginal\", \"simple_gat_eigen_marginal\",\"gt\", \"gt_eigen\", \"gt_eigen_cycles\",\"gt_marginal\", \"gt_eigen_marginal\",\"gt_eigen_marginal_cycles\"]\n",
    "\n",
    "for config in configs:\n",
    "    if config == \"simple\":\n",
    "        average_er_train_loss[config] = np.mean([er_train_loss_list[f\"{config}_{i}\"] for i in range(trial_num)])\n",
    "        average_er_val_loss[config] = np.mean([er_val_loss_list[f\"{config}_{i}\"] for i in range(trial_num)])\n",
    "        average_er_train_loss[config] = [average_er_train_loss[config]] * 100\n",
    "        average_er_val_loss[config] = [average_er_val_loss[config]] * 100\n",
    "        average_train_loss[config] = np.mean([train_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "        average_val_loss[config] = np.mean([val_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "        std_train_loss[config] = np.std([train_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "        std_val_loss[config] = np.std([val_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "    elif config in [\"simple_gcn_eigen_cycle\", \"simple_gat_eigen_cycle\",\"simple_gcn_eigen_cycle_marginal\",\"simple_gat_eigen_cycle_marginal\"]:\n",
    "        average_train_loss[config] = np.mean([train_loss_list[f\"{config}{i}\"] for i in range(trial_num)], axis=0)\n",
    "        average_val_loss[config] = np.mean([val_loss_list[f\"{config}{i}\"] for i in range(trial_num)], axis=0)\n",
    "        std_train_loss[config] = np.std([train_loss_list[f\"{config}{i}\"] for i in range(trial_num)], axis=0)\n",
    "        std_val_loss[config] = np.std([val_loss_list[f\"{config}{i}\"] for i in range(trial_num)], axis=0)\n",
    "    else: \n",
    "        average_train_loss[config] = np.mean([train_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "        average_val_loss[config] = np.mean([val_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "        std_train_loss[config] = np.std([train_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "        std_val_loss[config] = np.std([val_loss_list[f\"{config}_{i}\"] for i in range(trial_num)], axis=0)\n",
    "    \n",
    "\n",
    "# Plotting function\n",
    "def plot_losses(config):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(average_train_loss[config], label=\"train_loss\", color='blue')\n",
    "    \n",
    "    plt.fill_between(range(len(average_train_loss[config])),\n",
    "                     average_train_loss[config] - std_train_loss[config],\n",
    "                     average_train_loss[config] + std_train_loss[config], color='blue', alpha=0.3)\n",
    "    plt.plot(average_val_loss[config], label=\"val_loss\", color='green')\n",
    "    plt.fill_between(range(len(average_val_loss[config])),\n",
    "                     average_val_loss[config] - std_val_loss[config],\n",
    "                     average_val_loss[config] + std_val_loss[config], color='green', alpha=0.3)\n",
    "\n",
    "    plt.plot(np.asarray(average_er_train_loss[\"simple\"][:len(average_train_loss[config])])-0.001, label=\"er_train_loss\", color='red')\n",
    "    plt.plot(average_er_val_loss[\"simple\"][:len(average_train_loss[config])], label=\"er_val_loss\", color='orange')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss with Variations for simple_marginal')\n",
    "    plt.xlim(0,20)\n",
    "    plt.ylim(0.1, 0.14)\n",
    "    plt.show()\n",
    "\n",
    "# Plot losses for each configuration\n",
    "for config in configs:\n",
    "    plot_losses(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Evaluating generated graph* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MMD of clustering coeffecing and degree\n",
    "\n",
    "dict_stat_fn = {\n",
    "    \"degree\": get_degree_hist_array,\n",
    "    \"clustering\": get_clustering_hist_array,\n",
    "}\n",
    "\n",
    "mmd_dict={}\n",
    "mmd_dict = {f\"degree_{config}\": [] for config in configs}\n",
    "mmd_dict.update({f\"clustering_{config}\": [] for config in configs})\n",
    "\n",
    "\n",
    "for i in range(trial_num):\n",
    "  for configuration in configs:\n",
    "    graphs_to_get_result={}\n",
    "    graphs_to_get_result[configuration] = graphs_dict[f\"{configuration}{i}\"]\n",
    "    final_results = utils.QuantitativeResults(dict_stat_fn=dict_stat_fn, train_dataset=graphs_dict[f\"train_{i}\"], test_dataset=graphs_dict[f\"test_{i}\"])\n",
    "    for gen_method, gen_dist in graphs_to_get_result.items():\n",
    "        final_results.add_results(gen_method, gen_dist)\n",
    "        results_df = final_results.show_table()\n",
    "        mmd_values = results_df.loc[:, results_df.columns != 'ref']\n",
    "        mmd_dict[f\"degree_{configuration}\"].append(mmd_values.loc[\"degree\", gen_method])\n",
    "        mmd_dict[f\"clustering_{configuration}\"].append(mmd_values.loc[\"clustering\", gen_method])\n",
    "        \n",
    "for config in configs:\n",
    "    mmd_dict[f\"degree_{config}\"] = np.array(mmd_dict[f\"degree_{config}\"])\n",
    "    mmd_dict[f\"clustering_{config}\"] = np.array(mmd_dict[f\"clustering_{config}\"])\n",
    "    mmd_dict[f\"degree_{config}\"] = hmean(mmd_dict[f\"degree_{config}\"])\n",
    "    mmd_dict[f\"clustering_{config}\"] = hmean(mmd_dict[f\"clustering_{config}\"])\n",
    "    print(f\"{config} degree: {mmd_dict[f'degree_{config}']:.2f}\")\n",
    "    print(f\"{config} clustering: {mmd_dict[f'clustering_{config}']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov statistics of eigenvalue distribution\n",
    "\n",
    "ks_eigenvalue_dict={}\n",
    "ks_eigenvalue_dict = {f\"{config}\": [] for config in configs}\n",
    "for i in range(5):\n",
    "  train_dataloader = utils.nx_list_to_dataloader(graphs_dict[f\"train_{i}\"], bs=250)\n",
    "  for configuration in configs:\n",
    "    eigen_feats = EigenFeatures(mode='all')\n",
    "    generated_dataloader=utils.nx_list_to_dataloader(graphs_dict[f\"{configuration}{i}\"], bs=100)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        unmasked_holder, node_mask = to_dense(batch)\n",
    "        eigen_feature_train = eigen_feats(unmasked_holder.E, node_mask)[1]\n",
    "    eigen_feature_train=eigen_feature_train.reshape(-1)\n",
    "    for batch in generated_dataloader:\n",
    "        unmasked_holder, node_mask = to_dense(batch)\n",
    "        eigen_feature_generated = eigen_feats(unmasked_holder.E, node_mask)[1]\n",
    "    eigen_feature_generated=eigen_feature_generated.reshape(-1)\n",
    "\n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, ks_p_value = ks_2samp(eigen_feature_train, eigen_feature_generated)\n",
    "    ks_eigenvalue_dict[configuration].append(ks_stat)\n",
    "\n",
    "\n",
    "# Assuming ks_eigenvalue_dict is already populated\n",
    "for config in configs:\n",
    "    ks_eigenvalue_dict[config] = np.array(ks_eigenvalue_dict[config])\n",
    "    harmonic_mean = hmean(ks_eigenvalue_dict[config])\n",
    "    print(f\"{config} ks: {harmonic_mean:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validness, Unique and Novelty\n",
    "\n",
    "valid={}\n",
    "unique={}\n",
    "novel={}\n",
    "valid = {f\"{config}\": [] for config in configs}\n",
    "unique = {f\"{config}\": [] for config in configs}\n",
    "novel = {f\"{config}\": [] for config in configs}\n",
    "\n",
    "\n",
    "for i in range(trial_num):\n",
    "  for configuration in configs:\n",
    "    valid[configuration].append(count_valid_graphs(graphs_dict[f\"{configuration}{i}\"]))\n",
    "    unique[configuration].append(count_unique_and_novel_graphs(graphs_dict[f\"{configuration}{i}\"], graphs_dict[f\"train_{i}\"])[0])\n",
    "    novel[configuration].append(count_unique_and_novel_graphs(graphs_dict[f\"{configuration}{i}\"], graphs_dict[f\"train_{i}\"])[1])\n",
    "\n",
    "\n",
    "for config in configs:\n",
    "    valid[config] = np.array(valid[config])\n",
    "    unique[config] = np.array(unique[config])\n",
    "    novel[config] = np.array(novel[config])\n",
    "    harmonic_mean_valid = hmean(valid[config])\n",
    "    harmonic_mean_unique = hmean(unique[config])\n",
    "    harmonic_mean_novel = hmean(novel[config])\n",
    "    print(f\"{config} valid: {harmonic_mean_valid/100:.2f}\")\n",
    "    print(f\"{config} unique: {harmonic_mean_unique/100:.2f}\")\n",
    "    print(f\"{config} novel: {harmonic_mean_novel/100:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
